WEBVTT

00:04:04.000 --> 00:04:10.000
Sure to carbon on earth is in many different reservoirs.

00:04:10.000 --> 00:04:12.000
On the top we've got the atmosphere.

00:04:12.000 --> 00:04:19.000
One of quite a bit of worry, of course, so there's in total around 750 Giga tone.

00:04:19.000 --> 00:04:24.000
And that carbon is recycled quite quickly it's it's.

00:04:24.000 --> 00:04:40.000
Yeah, it's cycles around within years or so, then you've also got about similar amount of carbon stored in plants for example, trees, that will live up to a few hundred years.

00:04:40.000 --> 00:04:54.000
But many of those just live one year, then below that we've got the soils and Pete deposits quite a large amount of carbon in there and they're that carbon is stored in.

00:04:54.000 --> 00:05:00.000
So away from the atmosphere for up to say 10,000 years or so, yeah.

00:05:00.000 --> 00:05:14.000
Then of course we've got the fossil fuels, which have been away from the atmosphere for millions of years are essentially ready carbon free, and are now being pushed into the atmosphere.

00:05:14.000 --> 00:05:18.000
Then, about half of the carbon is actually in the ocean.

00:05:18.000 --> 00:05:41.000
Yeah. So, in the surface ocean, there is around 1000 Giga tons of carbon. And that gets recycled. And so through communication with the atmosphere for outgassing and things on scale over on forums hundred to 1000 years, then below that we've got the deep

00:05:41.000 --> 00:05:58.000
is even further, isolated from the atmosphere. And that has say throughput time or second time around 2000 to 2000 years and it depends a bit on where you are on Earth.

00:05:58.000 --> 00:06:12.000
And so, and so from the pre course survey that there's at least a few people are looking at Ocean sentiment course, so damn marine ready carbon is quite important.

00:06:12.000 --> 00:06:29.000
And as bit of an issue in that say on average, you're ready to carbon age from any marine material will be too old by, on average, around 500 520 years or so, but it's not a constant average just changes, it can change over time.

00:06:29.000 --> 00:06:47.000
And also certainly will also change depending on the location where you are on Earth, because there's places where old water that has been away from the atmosphere for thousands of years, is going through upwelling, and then causing carbon to be admitted,

00:06:47.000 --> 00:06:55.000
which is relatively relatively less ready car running this.

00:06:55.000 --> 00:07:06.000
And for example, so one region where there's lots of old carbon barriers lots of dilution of carbon is around the Antarctic the Southern Ocean.

00:07:06.000 --> 00:07:11.000
And if you are if you have material from there.

00:07:11.000 --> 00:07:13.000
The way.

00:07:13.000 --> 00:07:19.000
Sorry. Oh,

00:07:19.000 --> 00:07:20.000
sorry.

00:07:20.000 --> 00:07:22.000
No worries.

00:07:22.000 --> 00:07:40.000
So, animals, organisms that live around the Southern Ocean and eat carbon from there. And if you would catch them now they would really carbonate them they would show quite an old age, for example, during the Scott boater expedition.

00:07:40.000 --> 00:07:57.000
They got some hundred some animals and if you read a carbon date that penguin that I found apparent age is 1,390% 40, and a seal is about the same age.

00:07:57.000 --> 00:08:20.000
So there's this additional local, regional reservoir effects and decision has to do quite a bit with the global conveyor belts were around green limbs and quite fresh water is pushed downwards and has been transferred across the earth, and over scale

00:08:20.000 --> 00:08:23.000
of 5000 2000 years.

00:08:23.000 --> 00:08:40.000
And when. During that time, they rated carbon in those waters, when us away from the ocean and build us away from the atmosphere, it will then decay, gradually, some parts of Oceans they will be quite an age offsets.

00:08:40.000 --> 00:08:45.000
Yeah, we have to be aware of that and have to correct for that.

00:08:45.000 --> 00:09:04.000
And you can do that using for example the marine database which is hosted in Belfast by polar Reimer and Ron Reimer and and what it does, it has lots and lots of data points of shells, that are found on the beach, for example.

00:09:04.000 --> 00:09:16.000
So quite recent sales or sales different collected for them by museums, media, and before the 1950s, so before this, the boundaries.

00:09:16.000 --> 00:09:28.000
So we have shells have known more or less known tender age, and we measured already carbonation that gives me an idea of at certain locations, what is local.

00:09:28.000 --> 00:09:31.000
Additional retroviral effect is.

00:09:31.000 --> 00:09:38.000
So you can click for example, on one countries I Northern Ireland here if you happen to be here.

00:09:38.000 --> 00:09:50.000
There will be a few data points of shells collected. And that will tell you them say some estimates of this additional local restaurant effects.

00:09:50.000 --> 00:09:53.000
Again this this can change over time as well.

00:09:53.000 --> 00:10:01.000
But yeah, that's just something that you have to take into account, and will also increase your uncertainty.

00:10:01.000 --> 00:10:02.000
Right.

00:10:02.000 --> 00:10:19.000
Not only marine dates have this thing of have a reservoir effect, but also legs, certainly also in legs that these ready carbon offsets can certainly be there and they can also vary a lot over the core depth.

00:10:19.000 --> 00:10:33.000
It's not only in legs that are in regions where desert covenant bedrock the bedrock, but can also be other ways for old carbon to end up into the lake, big problem.

00:10:33.000 --> 00:10:38.000
I've worked on several lakes, where sometimes the.

00:10:38.000 --> 00:10:53.000
Then, this is this age offsets sometimes four or five years and then 3000 and then it says can can vary quite considerably. And it's not always clear what the reason is behind these.

00:10:53.000 --> 00:11:11.000
These offsets and their changes, but could be in some cases you could have a lake where there's cold soil around it, and that's occasionally, washed in, and then as carbon that is say a few hundred years old, into the lake.

00:11:11.000 --> 00:11:18.000
Understand perhaps them sentiments, or the could be other reasons.

00:11:18.000 --> 00:11:28.000
Could revoke access to or other reasons. Yeah, so do not think that because if you're like, has no carbonates then already carbon date should be fine.

00:11:28.000 --> 00:11:31.000
There could be many other reasons whether or not fine.

00:11:31.000 --> 00:11:43.000
And you choke. If you do work on Lake sediments. You also need to check for the correctness of your settlement. If you're using Belk settlement dates.

00:11:43.000 --> 00:11:54.000
You can use all you have to check how reliable they are, for example by measuring the surface sediments, if you know that it is from a recent ages.

00:11:54.000 --> 00:12:02.000
If you don't measure it and get 1000 year old age, that you know you have to apply around 2000 years angel said.

00:12:02.000 --> 00:12:21.000
You can also use pair dates where you have a single depth, you measure both a Belk date, and for example Poland, or charcoal or non ready current date for example you can use different layers of known tender age or lead to 10 dating, and then have some

00:12:21.000 --> 00:12:28.000
dog settlement dating as well, to see what is offset could be and how it can change over time.

00:12:28.000 --> 00:12:30.000
Yeah.

00:12:30.000 --> 00:12:31.000
All right.

00:12:31.000 --> 00:12:44.000
No, he also look at whether the target event is data is one look too much into this, but one issue is that if you have material.

00:12:44.000 --> 00:12:51.000
For example, a piece of junk have said sediment and you want to get a date for it.

00:12:51.000 --> 00:12:58.000
I don't know how much you would generally pay for single dates, but will be around.

00:12:58.000 --> 00:13:03.000
$400 a selfie four or $500.

00:13:03.000 --> 00:13:22.000
So, you don't really often want to spend money, lots of money to do replicate dating, but that's just the types of things that step radiocarbon labs do by sending around material of exactly well material, one piece of material, and then chop it up in

00:13:22.000 --> 00:13:31.000
to say 100, different samples and send them all across the road to different radiocarbon dabs, and then let them dated and report vector results.

00:13:31.000 --> 00:13:39.000
And that's done for here for a piece of boot, which is Denver data so we know what the xxH should be.

00:13:39.000 --> 00:13:49.000
But we don't tell that to the right of Kyra maps. And then over the cabinets measure sample gets an age report to age.

00:13:49.000 --> 00:13:59.000
In this case here, we've got the standard would which is then returned the age I returned. You can see that no, not a single lab agrees on the age. Yeah.

00:13:59.000 --> 00:14:10.000
Each level reports, slightly different mean and slightly different. aero bars. And there's also I think three or four naps that don't even fit on this graph, they're not plotted.

00:14:10.000 --> 00:14:20.000
Yeah. Some labs, give very very high proceeding dates and others much much larger uncertainties.

00:14:20.000 --> 00:14:35.000
So, one message here is that you shouldn't go to different labs. Of course you get scattered, but that's not really the message I want to get across because I think it is, it is important to go to different labs, see that the best methods are comparable

00:14:35.000 --> 00:14:52.000
and the results are comparable was also even if you stick, stay within the same lab, and measure the same material several times, which is what we do in Belfast we have several standards that, that have we have made once, and that we keep on ready carbon

00:14:52.000 --> 00:15:02.000
dating in order to have some standards in our data and process to make sure that the machine is is not drifting away.

00:15:02.000 --> 00:15:10.000
Yeah. So even for the standard there will be quite a some some change over time. Yeah.

00:15:10.000 --> 00:15:23.000
So, each time you send in material for dating, you get a number, and it's a very expensive number, but only a number it's only estimates, and next time you'll send the same material you might get a different age.

00:15:23.000 --> 00:15:26.000
So there's quite a bit different.

00:15:26.000 --> 00:15:41.000
So we just saw this date comes with a lab error. And we should not neglect that. So it's just an estimate, it's an expensive estimate. And, yeah, it's not a perfect age.

00:15:41.000 --> 00:15:43.000
Certainly not.

00:15:43.000 --> 00:15:57.000
And if we don't think about possible source of contamination, especially for older ages contamination with modern carbon can be a disaster.

00:15:57.000 --> 00:16:16.000
So imagine that you have a if you have like a piece of sediment, for example, we had a real age is a 50,000 years old, and the real already carbon ages 1500 years old, but you're working on the sample, and by accident, a little hair or a flake of skin

00:16:16.000 --> 00:16:20.000
falls into your sample, and you don't see it happening.

00:16:20.000 --> 00:16:24.000
And it's contaminates your sample with 1%.

00:16:24.000 --> 00:16:45.000
Modern carbon. Yeah, so 99% of your sample is of the age of the rate of carbon is of the he wants to be. But you have contamination with 1%. Modern carbon, then your parents age on this access will be around 3334 or five years old.

00:16:45.000 --> 00:16:46.000
Yeah.

00:16:46.000 --> 00:17:07.000
So this huge impacts of tiny amounts invisible amounts of contamination can have enormous impacts, especially on all the dates. And if you think back to that animation with all the little yellow dots that disappear over time, men were at ages of four

00:17:07.000 --> 00:17:21.000
yeah beyond 3040 50,000 years ago, there's so few ready government items left, that if you add a contemplation just a few radio carbon atoms, your sample will be much wrong.

00:17:21.000 --> 00:17:24.000
Yeah.

00:17:24.000 --> 00:17:44.000
And if you have so 1% on termination you get say are you real he gets off by several 10s of thousands years, if you have say 10% condemnation which is still, what I will be hopefully visible, you would end up with ages, less than 20,000 years old.

00:17:44.000 --> 00:17:47.000
Now imagine that

00:17:47.000 --> 00:17:59.000
there's a quick question in the chat about whether there is partitioning or fraction ation of carbon isotope in charcoal between what's been burned off and what's left behind.

00:17:59.000 --> 00:18:02.000
Or in general how that works.

00:18:02.000 --> 00:18:05.000
Yes, there is.

00:18:05.000 --> 00:18:11.000
Say, so you have see 12 and 13 and see 14 and there will be a bit of.

00:18:11.000 --> 00:18:19.000
Whenever there's any chemical reaction happening. This will happen to bit slower for heavier atoms.

00:18:19.000 --> 00:18:38.000
So, you will have some, some biases happening that way. But the good thing is that we measure not only see 14 but also see 13 nc 12, and any friction nation that will happen will happen, about twice as much for see 14 didn't foresee 13.

00:18:38.000 --> 00:18:57.000
That's why we also measure c 13 and correct for that bias, so that that's discrimination. So, if we measure how much discrimination has happened against see 14, you can then estimate how much it should it should have been for see 14, so that we do correct

00:18:57.000 --> 00:19:00.000
for that.

00:19:00.000 --> 00:19:04.000
I hope that answers the question.

00:19:04.000 --> 00:19:22.000
Now I mentioned that you have material, which is contaminated by 1% personal always 1%, it's 1% plus or minus something. Yeah, sometimes it's point 8% in another sample, maybe 1.2% so this source of contamination is not constant changes bits randomly,

00:19:22.000 --> 00:19:30.000
over time, but always is, on average around 1% and dusty black dots here.

00:19:30.000 --> 00:19:49.000
Yeah. So, if you have 1% contamination but that's better perfect rating amount in absolute terms it doesn't really do much to dates that are say quite reasons, but to work all the dates, it's kind of have a huge impact on these days.

00:19:49.000 --> 00:19:54.000
So yeah, this is something that's is causing me lots of trouble at the moment.

00:19:54.000 --> 00:20:07.000
But yeah, so it's optional thing to to consider, especially when you're looking at all the dates, but also this impacts can have large yeah it can have large impact, even on the relative dates, if that makes sense.

00:20:07.000 --> 00:20:11.000
Yeah.

00:20:11.000 --> 00:20:22.000
If you have contamination with all the carbon then that's less of an issue, but especially, all of a sudden modern carbon can be problematic.

00:20:22.000 --> 00:20:33.000
Right, let's move on to calibrate to calibration again. So, if we just open our again. We have already loaded.

00:20:33.000 --> 00:20:54.000
The Intel, our package. And that comes to the function called calibrate originally that was in the clam package but now it's in the Install button so if we simply go to the terminal and type calibrate, And then, open bracket close bracket.

00:20:54.000 --> 00:21:05.000
Then

00:21:05.000 --> 00:21:09.000
microphone open baby crying.

00:21:09.000 --> 00:21:12.000
Thank you.

00:21:12.000 --> 00:21:22.000
This is calibrating a standards of day to day life and not break arm date of 2450, plus or minus 50.

00:21:22.000 --> 00:21:28.000
And I like that a lot because that's where the calibration curve here in green, the integral 20 curve has a plateau.

00:21:28.000 --> 00:21:40.000
It has to be a period in time when in Europe at least as lots of things happening in archaeology. So there's lots of radiocarbon dates that span, that are from around that time.

00:21:40.000 --> 00:21:55.000
It's also it's a plateau so if you have a date of 2450 uncultivated hears it could actually stemmed from a period of a fee and a half centuries yourself.

00:21:55.000 --> 00:22:11.000
At least if you have single dates. If you start to include more information multiple dates and some deposition models we can actually use these vehicles to get very precise ages, but for single dates, this can be really problematic.

00:22:11.000 --> 00:22:12.000
Yeah.

00:22:12.000 --> 00:22:22.000
So what we see here in this graph is in red, we've got the date of the data, we want to calibrate.

00:22:22.000 --> 00:22:26.000
Then we've got this

00:22:26.000 --> 00:22:46.000
Goshen shape the bell shaped curve on this on this axis which is the uncultivated era distribution which we think that's if we have a date of two 450 plus minus 50, you can assume that it will then be distributed as normal distribution.

00:22:46.000 --> 00:22:58.000
Then we use the calibration curve, which is green one to translate the radiocarbon years into calendar year, because that's what we're interested in. Yeah, we were not interested in carbon years.

00:22:58.000 --> 00:23:04.000
He wants to know what the calendar years our of our samples. Yeah.

00:23:04.000 --> 00:23:16.000
So, generally, what is generally used is that we assume that, under rated garbage kill our samples are distributed, like a normal distribution.

00:23:16.000 --> 00:23:30.000
Actually, bacon uses student t distribution which is very similar to normal distribution, but has slightly wider tails, so it can accommodate more scatter will come to that later.

00:23:30.000 --> 00:23:38.000
Yeah. So, yeah, could you return to that previous graph for just a second.

00:23:38.000 --> 00:23:56.000
This one, and I know, yeah I know you're going to talk about this a little bit more but I wanted to point out that the, that probability distribution function, the sort of the access for it is invisible, it's, it's not shown on there.

00:23:56.000 --> 00:24:07.000
But what that is a probability of that date that radiocarbon date, being each of those calendar ages.

00:24:07.000 --> 00:24:20.000
Yeah, that's it. This is very confusing at first but I think this is if you get this then you will understand a lot more about radiocarbon calibration, so stay tuned.

00:24:20.000 --> 00:24:21.000
Yeah.

00:24:21.000 --> 00:24:31.000
Indeed, so you're right stairs is very confusing because what we see here actually not to access but as for access in this graph, two of which are invisible.

00:24:31.000 --> 00:24:43.000
Yeah. So here we've got the distribution, the probability distribution which is going this way. And it's plotted on an invisible, additional access.

00:24:43.000 --> 00:24:54.000
Yeah, so we should not interpret this thing in terms of these have this access. It's an invisible, it has an invisible additional access here for uncultivated date.

00:24:54.000 --> 00:25:08.000
And similarly, the calibrated probability distribution has additional uncluttered access, which is not related to the red ready carbon age here.

00:25:08.000 --> 00:25:11.000
Yeah.

00:25:11.000 --> 00:25:30.000
What it is is simply this graph so you've got your measurement here, and we assume it is normally distributors and given this measurement, we can ask for any value for example 2200, how likely that value is given our date, even our measurement.

00:25:30.000 --> 00:25:48.000
And if you're far away from the measurement, it's very unlikely that those values belong to that measurement. But as you climb the hill. We reach very likely value so if you will between 2400 to 500 those values are very likely given this date yet so

00:25:48.000 --> 00:25:52.000
So we're very high up to possibilities to distribution.

00:25:52.000 --> 00:26:00.000
So this is a very important concept, and we generally the axis here, we generally don't really got it.

00:26:00.000 --> 00:26:22.000
All we know a bunch of know is that this entire area seems to one, and the value here isn't really that important doesn't really tell us that much, but so when we got when we got to calibrate dates then, yeah, distributions are plotted on invisible additional

00:26:22.000 --> 00:26:24.000
access.

00:26:24.000 --> 00:26:26.000
Yeah.

00:26:26.000 --> 00:26:36.000
Alright, so let's go back to our then we're going to put this code here the blue code into our.

00:26:36.000 --> 00:26:42.000
So, If we want to look at normal distribution.

00:26:42.000 --> 00:27:01.000
We can ask for any value, we can ask how likely that value is for assuming a normal distribution. For example, if our measurement is 130 plus minus 20, we can ask how likely the value of 125 years.

00:27:01.000 --> 00:27:11.000
Yeah, so we can ask what is the normal distribution, the norm, open bracket of devalue 125.

00:27:11.000 --> 00:27:19.000
Given that the measurement is one of the 30 plus or minus 20.

00:27:19.000 --> 00:27:31.000
So we can ask are so assuming this distribution, how, what is the likelihood of probability of the actual value being one of them 25.

00:27:31.000 --> 00:27:38.000
And it tells us that's CHEER UP 101933341.

00:27:38.000 --> 00:27:52.000
So, 1.9% chance, probability of one of them 25 being the actual year of the updates or the actual value of that measurement.

00:27:52.000 --> 00:27:54.000
So quite low. Yeah.

00:27:54.000 --> 00:28:04.000
But we can also just return back to the original command we can also ask how likely is then the value of one of the 30.

00:28:04.000 --> 00:28:10.000
Given that the measurement is 130 plus or minus 20.

00:28:10.000 --> 00:28:33.000
And so about savings, but 19.9% now sorry, 1.99%. So 2% probability. Yeah, so the individual values, still have very low value of, very low probabilities, but that's because this is a quite a wide uncertainty took 20 years lab and 30 translates to say

00:28:33.000 --> 00:28:52.000
100 years show total distribution. So what we can do is we can go and ask what the probability is not for one valuable for a whole range of values, for example, we can get a new parameter, or variable called x course, why not.

00:28:52.000 --> 00:28:53.000
And we assigned.

00:28:53.000 --> 00:29:01.000
Do a little arrow thing again. And we give it up the numbers, zero to 250.

00:29:01.000 --> 00:29:04.000
Right.

00:29:04.000 --> 00:29:13.000
So then you have a parameter, or variable x. And if it just asked for x is just gives us the numbers.

00:29:13.000 --> 00:29:16.000
Zero to 250.

00:29:16.000 --> 00:29:19.000
Okay.

00:29:19.000 --> 00:29:33.000
And now we can ask, we can say, No, give me the probability for each of those values given a measurement of 120. So we can say x.

00:29:33.000 --> 00:29:40.000
And then we go back to 1030, plus minus 20.

00:29:40.000 --> 00:29:42.000
And then you get.

00:29:42.000 --> 00:29:52.000
I lost like 250 values for the probability distributions Yeah, for, for those x values.

00:29:52.000 --> 00:29:59.000
And we can actually better to debt as another variable called props for abilities.

00:29:59.000 --> 00:30:01.000
So probabilities.

00:30:01.000 --> 00:30:14.000
We that new variable be assigned the value of the norm x, comma, 130, comma, 20.

00:30:14.000 --> 00:30:27.000
Now we can combine the two. The mean bind them into columns, very first MX at the first column. And then we have a second column.

00:30:27.000 --> 00:30:37.000
And then we get a list of two columns there for each of the values for x, we've got the corresponding probability.

00:30:37.000 --> 00:30:38.000
Yeah.

00:30:38.000 --> 00:30:50.000
And so each of those values are very unlikely, but if we actually combine all the props together, all those 250 values.

00:30:50.000 --> 00:30:53.000
To add up to one.

00:30:53.000 --> 00:31:00.000
Yeah, so this whole area for distribution as up to one. So, we're still good.

00:31:00.000 --> 00:31:18.000
Now we can plot these guys as well we can say, simply say plot breakdown bracket, then x on the x axis and probes on the y axis, and a close bracket.

00:31:18.000 --> 00:31:30.000
And that gives us this horrible plus with all those little dots, because by default, are will clubs on our values as dots and not in a line.

00:31:30.000 --> 00:31:40.000
So we can go back and tell us to have a type, it should use line graph a line graph.

00:31:40.000 --> 00:31:53.000
So now we have for each of the values, zero to 250. We have to probability going up to 2% or so, and receive a normal distribution.

00:31:53.000 --> 00:32:08.000
Yeah, so we can see each value has a certain probability for that measurement. Yeah, so that measurement is only measurement and we can then ask, What if the value was really hundred.

00:32:08.000 --> 00:32:13.000
Or what if it was really 200, we can calculate the probability. Yeah.

00:32:13.000 --> 00:32:14.000
Okay.

00:32:14.000 --> 00:32:17.000
Good.

00:32:17.000 --> 00:32:19.000
All right.

00:32:19.000 --> 00:32:26.000
You can also use the same approach to make a very rudimentary calibration.

00:32:26.000 --> 00:32:39.000
By invoking the coloration curve. So we have the coloration curve, which you can see by Mike making a variable called cc innovation curve.

00:32:39.000 --> 00:32:42.000
And again we assign something to it.

00:32:42.000 --> 00:32:59.000
And for that we we assign the function calibration curve shi shi shi curve, which by default, puts income 20 into that variable.

00:32:59.000 --> 00:33:05.000
So we now have variable cc which contains the whole calibration curve.

00:33:05.000 --> 00:33:10.000
So if you type CC, I guess.

00:33:10.000 --> 00:33:21.000
Nine 9501 lines pros, each of which has at first element, it has the calendar year.

00:33:21.000 --> 00:33:31.000
The second element is to radiocarbon year, and the third element is to uncertainty the error of deaths integral ready carbon here.

00:33:31.000 --> 00:33:34.000
Okay.

00:33:34.000 --> 00:33:49.000
Right, so our calendar years here in Chicago is the first column of this, of this CC. And we can do that by saying CC.

00:33:49.000 --> 00:34:05.000
And then we use square brackets, which just something about which column, we want to take. Yeah. So we say, cc square brackets comma one.

00:34:05.000 --> 00:34:19.000
And then we close the square bracket records that will assign the first column of this variable called cc into this new variable year dot info.

00:34:19.000 --> 00:34:41.000
And then we could do the same for radiocarbon years blowing to those Canada years to see 14 in Cal, that becomes CC. But then the second column. So the second column is the column for it already covering two years of the coloration curve.

00:34:41.000 --> 00:35:01.000
And now we can use those columns that variable dental calculates probability for each date for each year. So, we can go again, calculate or make a variable called props be assigned to it.

00:35:01.000 --> 00:35:05.000
The US again, the normal distribution, the norm.

00:35:05.000 --> 00:35:15.000
And we want to have the rate of carbon years as the values that want to test, see 14 dot info.

00:35:15.000 --> 00:35:26.000
And then we add the mean and standard deviation of the value of the measurement of interest, which is one on a 30 plus or minus 20.

00:35:26.000 --> 00:35:32.000
Yeah, so we have probes. as a variable which contains lots and lots of values.

00:35:32.000 --> 00:35:33.000
Yeah.

00:35:33.000 --> 00:35:37.000
9501 values.

00:35:37.000 --> 00:35:42.000
Now we can then drops.

00:35:42.000 --> 00:35:46.000
On the exhale uploads the years of Intel.

00:35:46.000 --> 00:35:52.000
And then a lot of props

00:35:52.000 --> 00:35:56.000
and

00:35:56.000 --> 00:36:07.000
and devil gators this horrible graph, which goes all the way back to 55,000 years ago and has this really sharp peak, which doesn't really make much sense.

00:36:07.000 --> 00:36:12.000
So what we'll do is we'll just adapt that access limits.

00:36:12.000 --> 00:36:24.000
So tell are the limits of the x axis should be between two values. So the minimum to left most value and the right most value.

00:36:24.000 --> 00:36:28.000
The minimum will be Jarrow the most recent age.

00:36:28.000 --> 00:36:31.000
And we have 300 Esther last age.

00:36:31.000 --> 00:36:38.000
And we also make it a nice line graph instead of these dots.

00:36:38.000 --> 00:36:41.000
Okay.

00:36:41.000 --> 00:36:51.000
And now, we have the calibrated distribution for the last few hundred years for that date 130% times 20.

00:36:51.000 --> 00:36:59.000
cc you don't need software, dedicated software to get your calibrated data you can actually calculate on yourself.

00:36:59.000 --> 00:37:09.000
Luckily enough we don't need to do that because there's software to work for you, for you. But this shows you yeah how, what's the process of this collaboration.

00:37:09.000 --> 00:37:22.000
Okay, so if you're still awake, less than look at how we go about calibrating a date. Yeah. So again, let's go back to this.

00:37:22.000 --> 00:37:31.000
Imagine that we have a date of 2450, and not worry, this is 2,400% minus 50.

00:37:31.000 --> 00:37:35.000
And then in green, you've got coloration curve. Okay.

00:37:35.000 --> 00:37:47.000
Again, we're assuming based on this measurements that we that this behaves as a normal distribution so that uncertainty is distributed distributed as normal distribution.

00:37:47.000 --> 00:37:55.000
And his normal distribution is placed is plotted on an additional access, or just drawn a line here. Yeah.

00:37:55.000 --> 00:37:59.000
Now, what we want to do is we have to compare.

00:37:59.000 --> 00:38:03.000
And we have to somehow combine

00:38:03.000 --> 00:38:18.000
this uncultivated unknown date with the coloration curve to get an idea of what are likely calendar ages for this date, because we're not interested in reading garden aged or interested in Canada ages.

00:38:18.000 --> 00:38:28.000
So what we do is we go to any single calendar here, for example, 2150.

00:38:28.000 --> 00:38:41.000
And then, shall we go to that calendar year. And then we check for Intel we go to the Intel curve and me check what the corresponding radiocarbon years for the coloration curve.

00:38:41.000 --> 00:38:44.000
So that's some, some value somewhere.

00:38:44.000 --> 00:38:56.000
And then we draw horizontal line to see for depths given radiocarbon year of the coloration curve, how far away it is from our measurement.

00:38:56.000 --> 00:39:14.000
And this case for 2001 of the 50 Cal dp the curse funding integral radiocarbon year is very far away from our measurement, in effect, it's so far away that we're actually in the valley of this whole probability mountain.

00:39:14.000 --> 00:39:33.000
So we have a zero probability that x yo radiocarbon age of that measurement is more than 2200. So what we do with them is transfer the height of that floods, to our calendar axis, and then we plotted on invisible, additional access.

00:39:33.000 --> 00:39:35.000
So in this case, you've got zero here.

00:39:35.000 --> 00:39:38.000
Yeah.

00:39:38.000 --> 00:39:52.000
Oh, that was a lot of work just for one camera here so we can do that automatically, of course, so we can simply go for one camera year, check the corresponding radiocarbon year, check if Hydra for religious devotion and draw that Providence distribution

00:39:52.000 --> 00:40:03.000
on the x axis, and then repeat for next year, go for it ready carbon year, find how far away is from the dates plotted to probability, etc.

00:40:03.000 --> 00:40:15.000
And in disguise who that 50 years spacing to for now, but we do that, no spacing. Soon, is perishing, or more close.

00:40:15.000 --> 00:40:16.000
Yeah.

00:40:16.000 --> 00:40:24.000
So now we're getting it can be used, which have ready carbon years of the congressional offices are just getting a bit likely. Yeah. And so we get a bit higher up the coloration probability curve and upload that heights.

00:40:24.000 --> 00:40:33.000
So we get a bit higher up the coloration probability curve, and the club that heights. On the kind of the scale.

00:40:33.000 --> 00:40:38.000
Now we're getting us coming years which has very likely ready carbon years.

00:40:38.000 --> 00:40:41.000
Given the date.

00:40:41.000 --> 00:40:51.000
So we can very likely value so we've got that on our calendar scale this invisible extra access.

00:40:51.000 --> 00:41:13.000
Yeah. So if it were to be all together in one room we would actually start drawing this on a piece of paper, and it's it's really confusing to try to really understand how this calibrated distribution is is made by pushing the date, through the calibration

00:41:13.000 --> 00:41:15.000
curve.

00:41:15.000 --> 00:41:30.000
The important bit is that we go from calendar years and for each calendar year, we asked, What's the corresponding interval, see 14 year and how likely is that see 14 year given dirty carbon dated for wanting to calibrate.

00:41:30.000 --> 00:41:33.000
Yeah.

00:41:33.000 --> 00:41:46.000
So we can do that a bit faster by calculating simply every time of the year. And now you can see how over time. So we're still it's very unlikely coming years, but we're reaching territory.

00:41:46.000 --> 00:41:50.000
You're getting higher I'm going to religious tuition.

00:41:50.000 --> 00:41:58.000
Staying quite high, but me getting a bit little bit lower, getting away from the ready carbon date.

00:41:58.000 --> 00:42:05.000
Then we get back, way back, and a way for always.

00:42:05.000 --> 00:42:19.000
Yeah. So that's how we get this weird asymmetric multimodal distribution, the calibrated distribution by pushing this radiocarbon date and there's no one has to achieve through the collaboration curve.

00:42:19.000 --> 00:42:22.000
Yeah.

00:42:22.000 --> 00:42:31.000
So, yeah, again if we just go back to our and type calibrate calibrate.

00:42:31.000 --> 00:42:40.000
You get this similar similar type of radiocarbon date. It's a slightly different data.

00:42:40.000 --> 00:42:48.000
And again, we're using the calibration curve and the coloration curve is simply

00:42:48.000 --> 00:43:04.000
era, or a table of three values of importance, Cal dp so he kind of years, the corresponding radiocarbon years, and then the air traffic type that GC becomes a calibration curve.

00:43:04.000 --> 00:43:10.000
And then if we just want to show the first few lines of deaths.

00:43:10.000 --> 00:43:26.000
That's variable UFC here that's four zero call dp number of corresponding ready to carbon years 199 plus minus 11 for year one LBP, it is 197 plus 11 for you too.

00:43:26.000 --> 00:43:29.000
It's fun and 511, etc etc.

00:43:29.000 --> 00:43:31.000
Okay.

00:43:31.000 --> 00:43:36.000
Right. So that's how you get those weird kind of ratings distributions.

00:43:36.000 --> 00:43:51.000
And we can also assume, instead of using the normal distribution we can use one student t, which is very similar to this normal distribution. But when you use for collaboration it actually results in slightly wider tails.

00:43:51.000 --> 00:44:06.000
So it really helps if we have some scattering dates, and then it will actually results in some areas which are still possible back to normal distribution already says like notice it's not possible.

00:44:06.000 --> 00:44:25.000
So, by using a student to distribution which actually makes a lot of sense given the amount of scatter that we see in real data carbonates using the student to distribution we can get actually quite, quite a good models and deal with scatter.

00:44:25.000 --> 00:44:35.000
Alright. Just one more thing in that often you see calibrated dates reported as that calibrated ranges.

00:44:35.000 --> 00:44:39.000
And what that is, it is the highest posterior density ranges.

00:44:39.000 --> 00:44:58.000
And they're conceptually made by lowering horizontal line and seeing lowering it until you reach a certain a certain amount of the curve that of the entire distribution that is covered by the gray areas.

00:44:58.000 --> 00:45:12.000
As you know, the line further and further more and more of the area, crushed by deadline is covered. So in this case here 68% is covered, which is one standard deviation.

00:45:12.000 --> 00:45:18.000
But nowadays most often we go to 95%, standard deviation ranges.

00:45:18.000 --> 00:45:25.000
And then your reports, those ranges Yeah, this one, this one, this one and that one.

00:45:25.000 --> 00:45:37.000
My street please an indigenous Yes. What does HPD stand for something probability distribution highest posterior density is posterior, not even close.

00:45:37.000 --> 00:45:50.000
austere density. Because there's many ways in which you can find in Michigan, clubs, and 95% range of such a beard horrible distribution. You could use.

00:45:50.000 --> 00:46:08.000
So you could eat of a two and a half percent of each side. But then you might not capture the highest piece. So, this value, this measure meant, or this way of calculating the highest posterior density ranges and shores, that the most likely bits of your

00:46:08.000 --> 00:46:14.000
calculator distribution are included it favors cakes.

00:46:14.000 --> 00:46:16.000
Yeah.

00:46:16.000 --> 00:46:34.000
Okay. But that's actually just for reporting your dates and dose ranges are not used in any way in the calibration or in age modeling software course it uses the entire distribution not just a single, single ranges that just for us humans to more or less

00:46:34.000 --> 00:46:40.000
make sense of the date but does not really used into software.

00:46:40.000 --> 00:46:48.000
Okay. And so there's also this thing of position versus Chrissy Metz you could have.

00:46:48.000 --> 00:47:00.000
If you have samples which are very much in one area. That's nice, because they're very precise, but doesn't necessarily mean that they're also accurate.

00:47:00.000 --> 00:47:02.000
Yeah.

00:47:02.000 --> 00:47:07.000
You want them to be like, on, on targets as well.

00:47:07.000 --> 00:47:13.000
So in this case here we have two distributions. Both of them are precise, but not accurate.

00:47:13.000 --> 00:47:24.000
This one here in the middle, we've got on average to accurate, but they're very wide of the mark so they're actually not very precise. And if you want to have both.

00:47:24.000 --> 00:47:30.000
Yeah, both accurate and precise.

00:47:30.000 --> 00:47:40.000
And the other thing so that actually leads us to all kinds of decisions that we're making when we treat dates and we calibrate dates. Yeah.

00:47:40.000 --> 00:47:51.000
So for example if we have a calibrated eight or some material which really is the year of 200, Cal dp.

00:47:51.000 --> 00:47:52.000
Yeah.

00:47:52.000 --> 00:48:04.000
And we, it is terrestrial material. We rated carbon dated. And if the material really is 200 years old, is truth gets

00:48:04.000 --> 00:48:09.000
terrestrial radio carbon age of around 160 or so.

00:48:09.000 --> 00:48:16.000
And if you can calibrate it using the correct coloration pair of the green month into 20, you get this calibrated distribution.

00:48:16.000 --> 00:48:31.000
And yea, the year 200 actually falls well within our range so our calibrated data is a good representation of the true age because the true age falls within the range there.

00:48:31.000 --> 00:48:45.000
If our material was Marine, so it was still 200 years old but it's actually marine and not terrestrial, then the corresponding ready cabin gear will be around 540 or so 530.

00:48:45.000 --> 00:49:01.000
And if we don't use the correct coloration curve to marine curve, then will again calibrated, and we get, again, a coloration correction distribution where the truth is still in there, so that's fine.

00:49:01.000 --> 00:49:16.000
But if by accident or material is Marine, but we are not aware of it I don't know why that would happen. But imagine that we use the wrong curve to calibrate our material, so it is actually marine and should have been data to the marine curve, but by

00:49:16.000 --> 00:49:21.000
accident, we used to run curve or some wrong assumptions.

00:49:21.000 --> 00:49:26.000
Then, are calibrated distribution will be very precise and nice.

00:49:26.000 --> 00:49:32.000
But it's totally wrong, it does not include a 200 at all. Yeah.

00:49:32.000 --> 00:49:51.000
Okay, so yeah we're running out of time so just wanted to go quickly to some introduction of ready, of age modeling and isn't order animation where you've got climates here, temperature that is very cold, and there's lots of ice is ice cap here and some

00:49:51.000 --> 00:50:10.000
beautiful ASCAP and very little limitation. Yeah. and our Lake, which accumulates sediments, over time, is very lights, it doesn't really do much but then as temperatures, get higher, the ice disappears, just more vegetation and more organic material

00:50:10.000 --> 00:50:14.000
entering the lake. Yeah. and just like keeps on accumulating.

00:50:14.000 --> 00:50:31.000
And after a few volunteers we come here and we core dislike, and try to interprets. What happened, and then try to reconstruct meditation, over time, and eyes over time and temperature over time, and that's that's our model for that it's very important

00:50:31.000 --> 00:50:40.000
to know when things happen so we, how to translate the depths of your core to time.

00:50:40.000 --> 00:50:50.000
So what we want, with a step modeling when we produce a step or what we want, we want to be. Get get very precise ages yes so the more precise the better at this test.

00:50:50.000 --> 00:50:59.000
Yeah, that would be really nice to have something no something that Decatur precision, or annual position and not a millennial precision.

00:50:59.000 --> 00:51:10.000
And why do we want to know the ages. That's because we want to put our citing to context we want to compare it with oversights to make sense of what happened when what happened first what happened later.

00:51:10.000 --> 00:51:17.000
We have to put some a common timescale. Otherwise, it cannot compare our different bits of information.

00:51:17.000 --> 00:51:28.000
But what. So what we are one does not preset perhaps compare with what we actually have, because in most cases, when we have a sentiment score we only have a few dates, along the score.

00:51:28.000 --> 00:51:34.000
yeah handful of dates perhaps might have too much money or not enough digital material.

00:51:34.000 --> 00:51:44.000
And those days are uncertainties to might be biased. There certainly will have scatter and it will be outliers. So that's problematic.

00:51:44.000 --> 00:52:05.000
So what we need is some way or some approach that can combine all this dating information and take care of the bias and the scatter and the outliers, to come up with a model, a time series or reconstruction of time series that is hopefully realistic and

00:52:05.000 --> 00:52:13.000
robust to the different assumption. And also, it's good at quantifying are uncertainties. Yeah.

00:52:13.000 --> 00:52:17.000
We're nearly out of time so I'm going to go quickly.

00:52:17.000 --> 00:52:31.000
So if we just go and go Google for an eight step model is just one of many demos, you could choose where we've got depth on the vertical scale and age, on the horizontal scale.

00:52:31.000 --> 00:52:39.000
And we've got a model. Yeah, curve going through some dates.

00:52:39.000 --> 00:52:43.000
And these dates. So there was a midpoint chosen.

00:52:43.000 --> 00:52:47.000
And the curve is going through that date and that's that's their model.

00:52:47.000 --> 00:53:04.000
But we've already seen that radiocarbon dates, especially when they're calibrated they end up often being multimodal, and yet so it's some it's quite difficult to choose which mid points, we should use question.

00:53:04.000 --> 00:53:09.000
And and also in this case to four days were rejected.

00:53:09.000 --> 00:53:21.000
That was because they didn't agree with order inflammation known from the area. It was pulling information accorded said, No, there's really governance cannot be correct, you have to reject them.

00:53:21.000 --> 00:53:27.000
And that happens, and things sometimes ready Cabernets can be wrong.

00:53:27.000 --> 00:53:41.000
And then also, they use the model of connecting neighboring dots of accepted dates, and that it could be but how do you connect us punch you just connect the dots or chaos and draw a straight line.

00:53:41.000 --> 00:53:56.000
And that's the types of things will be looking at. Next time, and also how to express the uncertainty. So there's no uncertainty expressed on this curve is just one curve, but we somehow have this uncertainty, we have to try to estimate that.

00:53:56.000 --> 00:54:05.000
OK, so the next session will look at first the theory and then a tutorial of random begun for a step modeling.

00:54:05.000 --> 00:54:12.000
And yeah, if there's any questions I'll be happy to answer them, if possible.

00:54:12.000 --> 00:54:18.000
Yes, that's me.

00:54:18.000 --> 00:54:31.000
Yeah, any questions at the end of our session here.

00:54:31.000 --> 00:54:33.000
I did put in the chat.

00:54:33.000 --> 00:54:45.000
The post session survey, so you can let us know whether there were things you were confused about if you have other questions you can ask them there and we'll address them.

00:54:45.000 --> 00:54:48.000
At the beginning of the next session.

00:54:48.000 --> 00:54:59.000
So, yeah, a Google Doc. Please fill it out. If you can, right now it's just a few questions, and.

00:54:59.000 --> 00:55:02.000
Yeah.

00:55:02.000 --> 00:55:07.000
I started

00:55:07.000 --> 00:55:13.000
talking for two hours will do that to you.

00:55:13.000 --> 00:55:26.000
Make sure that you guys have your. If you have your own data and your own course that you have, you know, make sure you have those ready for the subsequent sessions.

00:55:26.000 --> 00:55:31.000
And make sure you guys are in bacon running for next time.

00:55:31.000 --> 00:55:41.000
And we'll send out also about some information that might be on Martin's second session page about preparing your own data.

00:55:41.000 --> 00:55:56.000
It's not we'll send something out about it but I but I bet it is so it doesn't need to be anything super complicated but if you don't have your own data, we have there, there will be example data sets that you can play with.

00:55:56.000 --> 00:55:58.000
Yeah.

00:55:58.000 --> 00:56:15.000
Okay, so yeah, fill out the survey, please. I'll paste it in there again just for giggles and, or you can send me an email you all have my email address from many emails from me, so.

00:56:15.000 --> 00:56:18.000
Okay. Thanks everybody.

00:56:18.000 --> 00:56:22.000
So good to see everyone here. Sorry least.

00:56:22.000 --> 00:56:25.000
I was saying, thanking Martin.

00:56:25.000 --> 00:56:34.000
Martin, and his family, and we will see you back here in a week.

00:56:34.000 --> 00:56:36.000
Alright. Thanks so much.

00:56:36.000 --> 00:56:38.000
See you next week.

00:56:38.000 --> 00:56:40.000
Thank you.

00:56:40.000 --> 00:56:42.000
No.

00:56:42.000 --> 00:57:02.000
Thank you very much. That was great.

00:57:02.000 --> 00:57:04.000
Cool Martin oh that was great.

00:57:04.000 --> 00:57:06.000
Good.

00:57:06.000 --> 00:57:36.000
Like fundamental stuff, no it's I mean I always get that when I talk for a long time.

